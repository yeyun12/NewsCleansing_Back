# import os
# import time
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# from transformers.utils import logging


# logging.set_verbosity_info()

# # 0. í™˜ê²½ ë³€ìˆ˜ì—ì„œ HF í† í° ë¡œë“œ
# hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN")
# if not hf_token:
#     raise ValueError("âŒ HUGGINGFACE_HUB_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")

# # 1. ëª¨ë¸ ë¡œë”©
# model_id = "google/gemma-2-2b-it"

# quant_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.bfloat16,
# )

# # ğŸ”§ tokenizer ë¡œë“œ (trust_remote_code ê¼­ True)
# tokenizer = AutoTokenizer.from_pretrained(
#     model_id,
#     token=hf_token,
#     trust_remote_code=True
# )

# # ğŸ”§ model ë¡œë“œ (trust_remote_code ê¼­ True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     token=hf_token,
#     device_map="auto",
#     trust_remote_code=True,
#     quantization_config=quant_config
# )

# # ğŸ” ëª¨ë¸ config í™•ì¸ ë¡œê·¸ (ì„ íƒ ì‚¬í•­)
# print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.config.model_type}")

# # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
# system_prompt = """ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ ê°ì •ë¶„ì„ ì „ë¬¸ê°€ì•¼.
# ì£¼ì–´ì§„ ê¸°ì‚¬ë¥¼ ì½ê³  ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì¤˜:

# **ê°ì • ë¶„ë¥˜**: ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì  ì¤‘ í•˜ë‚˜  
# **ì‹ ë¢°ë„**: 0-100%  
# **ì£¼ìš” ê·¼ê±°**: íŒë‹¨ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…  
# **ê¸°ì‚¬ ì˜ë„**: ì–´ë–¤ ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ë¶„ì„í•˜ê³  ì „ë‹¬í•˜ë ¤ëŠ” ë©”ì„¸ì§€ë¥¼ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì¤˜.
# **ìš”ì•½**: ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.

# ì •í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•´ì¤˜. ë‚´ê°€ ì§€ì‹œí•˜ì§€ ì•Šì€ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆ.
# """

# # 2. ë¶„ì„ í•¨ìˆ˜
# def analyze_sentiment(content: str):
#     messages = [
#         {"role": "user", "content": f"{system_prompt}\n\në¶„ì„í•  ê¸°ì‚¬:\n{content}"}
#     ]
#     inputs = tokenizer.apply_chat_template(
#         messages,
#         tokenize=True,
#         add_generation_prompt=True,
#         return_tensors="pt"
#     ).to(model.device)

#     start_time = time.time()

#     with torch.no_grad():
#         outputs = model.generate(
#             inputs,
#             max_new_tokens=512,
#             temperature=0.7,
#             do_sample=True,
#             pad_token_id=tokenizer.eos_token_id
#         )

#     duration = round(time.time() - start_time, 2)

#     result = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
#     return {
#         "analysis": result,
#         "inference_time_sec": duration
#     }
